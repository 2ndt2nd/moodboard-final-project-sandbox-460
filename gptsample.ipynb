{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch torchvision clip-by-openai pillow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load CLIP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import clip\n",
    "from PIL import Image\n",
    "\n",
    "# Load the CLIP model (ViT-B/32 is the default)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Extract Image Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_image_features(image_path):\n",
    "    image = preprocess(Image.open(image_path)).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        image_features = model.encode_image(image)\n",
    "    return image_features / image_features.norm(dim=-1, keepdim=True)  # Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load images and compute embeddings\n",
    "image_folder = \"path/to/your/images\"\n",
    "image_features_dict = {}\n",
    "\n",
    "for filename in os.listdir(image_folder):\n",
    "    if filename.lower().endswith((\"png\", \"jpg\", \"jpeg\")):\n",
    "        image_path = os.path.join(image_folder, filename)\n",
    "        image_features_dict[filename] = extract_image_features(image_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Extract Text Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_features(text):\n",
    "    text_tokenized = clip.tokenize([text]).to(device)\n",
    "    with torch.no_grad():\n",
    "        text_features = model.encode_text(text_tokenized)\n",
    "    return text_features / text_features.norm(dim=-1, keepdim=True)  # Normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Compare Images and Text (Run Code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Load images and compute embeddings\n",
    "image_folder = \"path/to/your/images\"\n",
    "image_features_dict = {}\n",
    "\n",
    "for filename in os.listdir(image_folder):\n",
    "    if filename.lower().endswith((\"png\", \"jpg\", \"jpeg\")):\n",
    "        image_path = os.path.join(image_folder, filename)\n",
    "        image_features_dict[filename] = extract_image_features(image_path)\n",
    "\n",
    "# Search for images matching a text prompt\n",
    "query = \"fisheye perspective, black hair\"\n",
    "text_features = extract_text_features(query)\n",
    "\n",
    "# Compute cosine similarity\n",
    "similarities = {\n",
    "    img_name: torch.cosine_similarity(text_features, img_features, dim=-1).item()\n",
    "    for img_name, img_features in image_features_dict.items()\n",
    "}\n",
    "\n",
    "# Sort results by similarity\n",
    "sorted_images = sorted(similarities.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Display top matches\n",
    "for img, score in sorted_images[:5]:\n",
    "    print(f\"{img}: {score:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
